# Программное обеспечение устройств и систем вычислительной техники
# Лекция 2
## Правило обучения Хебба
1) Если 2 нейрона по обе стороны синапса активируются одновременно, прочность соединения возрастает
2) Если 2 нейрона по обе стороны синапса активируются асинхронно, то такой синапс ослабляется или отмирает.

Следствия
1) Зависит от времени
2) Локальность
3) Интерактивность
4) Корелляция

*Первое правило Хебба* - если сигнал персептрона неверен и равен нулю, то необходимо увеличить веса тех входов, на которые была подана единица.

*Второе правило Хебба* - если сигнал персептрона неверен и равен единице, то необходимо усеньшить веса тех входов, на которые была подана единица.

$$
w_{ij}(t+1) = w_{ij}(t) + nNET_iNET_j
$$
n - положительная константа, определяющая скорость обучения.
NET обычно вычисляется как взвешенная сумма входных сигналов нейрона:
$$
NET = \sum_{k} x_k w_k
$$
где \( x_k \) — входные сигналы, а \( w_k \) — соответствующие веса.

При обучении с учителем выходной сверяется с правильными данными, при без учителя ?проверяется кластеризация входных данных?

## Классификация
Задача классификации заключается в отнесении входных данных (образов) к одному из заранее определённых классов на основе их характеристик.

На плоскости мы отделяем одни точки и области от других линиями, в трехмерном пространстве плоскости и тд

При реальных задачах мы не сможем произвести такую предобработку данных чтобы появлялась линейная разделимость

## Персептрон Розенблата
Простейший вид нейронной сети, в основе лежит математическая модель восприятия мозга, состоящия из
- S нейронов - Сенсорные. Формирование входных данных
- A нейронов - ассоциативные. Для непосредственной обработки входных нейронов
- R-нейронов - реагирующие. Предназначены для передачи сигналов к другим нейронам и сетям. Имеют несколько входов и один выход.

## Обучение по Дельта правилу
Обучение на основе градиентного спуска

*Оптимизация в расчетах* - книжка про градиентные спуски от лектора

Отличия
1) вводится коэф n скорость бучения 0 < n < 1
2) искл некоторые действия, например обновление весовых коэф
3)...

На каждем шаге вычисляется ошибка
$e_j=d_j-y_j$

где $d_j$ - эталонное, $y_j$ - фактическое

таким образом формула такая:
$$
w_i(t+1)=w_i(t) + n*x_i* e_i
$$

Подходит для распознавания образов, например цифр или букв.

Для других задач такой персептроне не годится, вроде прогнозирование погоды, курсов акций, валют

Круг решаемых задач значительно расширяется, если научить персептрон выдавать не только бинарные сигналы, но и аналоговые, непрерывные. Появление персептронов с непрерывной пороговой функцией обусловило появление новых подходов к обучению. Например **квадратичная ошибка** обучения перцептрона. 

Градиент функции направлен в сторону меньшего возрастания. А поскольку мы ищем минимальную ошибку, то движемся в сторону, противоположную градиенту. Это называется **градиентный спуск**.

$F(x)=\frac{1}{1+e^{?}}$

## Входные данные
Могут быть представлены в бинарном виде например, и несколько параметров могут быть заданы одним двоичным числом.

# Лекция 3
## Функция исключающего ИЛИ
Ограниченность однослойного перцептрона - для некоторых задач (и их большинство) погрешность $\epsilon$ не удавалось снизить даже при большом количестве эпох t.

Одна из задач, которую не могут решить персептроны - ошибка исключающего ИЛИ
| x1 | x2 | y |
|-|-|-|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

Задача была - с помощью персептрона получать результат функции. Функцию при этом можно было воссоздать из И, ИЛИ, НЕ.

Ученые предложили геометрическую интерпретацию, где по одной оси x1 по другой x2, и получилось что невозможно подобрать такие веса, чтобы линия отделяла точки именно таким образом. А уравнение пороговой прямой для однослойного персептрона это $w_1x_1+w_2x_2=\theta$. А то что распознавать буквы и цифры латинского алфавита было возможно оказалось удачным совпадением, что она была линейно разделима. Получилось, что ученые бились над некоторыми другими задачами, которые не могли быть решены в принципе с помощью однослойного персептрона.

**Линейная разделимость** - нейрон с n двоичными входами может иметь $2^n$ различных входных образов, состоящих из нулей и единиц.Так как каждый входной образ может быть 1 или 0, но всего имеется $2^{2^n}$ функций от n переменных.

Позднее ограничение было преодолено добавлением дополнительных слоев, однако развитие нейросетей было заторможено, и те кто в то время ими занимался считались чудаками.

Если к персептрону добавить еще 1 слой, то он сможет решать задачу исключающего ИЛИ. Фактически многослойные персептроны могут проводить больше линий, ограничивая пространство.

Теоретический анализ многослойных персептронов труден, а обучение более долгое.

$$
OUT = (XW_1)W_2
$$
$$
(XW_1)W_2=X(W_1W_2)
$$

## Градиент
Градиент - частная производная от функции. Если найти градиент можно найти направление увеличения функции.

А метод градиентного спуска - исполльзование в обратную сторону, т.е. находим уменьшение функции, для минимизации ошибки.

Метод наискорейшего спуска отличается от других методов спуска (например с постоянным шагом) величиной шага.

Алгоритм
1) Задать размерность задачи оптимизации n, координаты начальной точки
2) Счетчик k=0
3) Определить направление вектора градиента целевой функции
4) Проверить условие окончания поиска
5)
6)

## Метод обратного распространения ошибки
$$
f(U)=\frac{1}{1+e^{-x}}
$$
или
$$
OUT=\frac{1}{1+e^{-NET}}
$$

# Лекция 4
## Рекурентные нейронные сети
Это сети с обратной связью. Их общая черта - сигнал с выходного или скрытого слоя передается на входной.

Изменение одного нейрона влияет на всю сеть. Для них не подходит ни обучение с учителем, ни без него. Все обучение сводится к расчету синапсов?.

Наиболее известные - сеть Хопфинга, сеть Хемминга. 

Сети Хопфинга
- Симметрия дуг
- Симметрия весов
- Бинарные входы

**Бинарная цепь Хопфильда** определяется симметричной матрицей с нулевыми диагональными элементами, вектором T порогов нейронов и знаковой функцией активации или выхода нейронов.
